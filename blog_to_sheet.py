import requests
from bs4 import BeautifulSoup
import gspread
from google.oauth2.service_account import Credentials
import datetime
import time
import re
import os

# --- è¨­å®š ---
BLOG_URL = "https://hisakobaab.exblog.jp/"

# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆID
SPREADSHEET_KEY = "1VJFQK3RWW1aH2FdH7P5it6EsgP4PmxKIlXdXDen7TzQ"

# ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆéµã®ãƒ‘ã‚¹
SERVICE_ACCOUNT_FILE = "/home/yasutoshi/projects/06.mini_keyboard/service_account.json" 

# æœ€æ–°ã®ä½•è¨˜äº‹åˆ†ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‹
LATEST_ARTICLE_LIMIT = 5

def get_soup(url):
    headers = {"User-Agent": "Mozilla/5.0 (RaspberryPi) AppleWebKit/537.36"}
    try:
        resp = requests.get(url, headers=headers)
        resp.raise_for_status()
        return BeautifulSoup(resp.content, "html.parser")
    except Exception as e:
        print(f"Error accessing {url}: {e}")
        return None

def parse_excite_date(date_text):
    """
    ãƒ–ãƒ­ã‚°ã®æ—¥ä»˜æ–‡å­—åˆ—ã‹ã‚‰ YYYY/MM/DD HH:MM:SS å½¢å¼ã‚’ä½œæˆã™ã‚‹
    å…¥åŠ›ä¾‹: "... at 2025-12-25 14:24"
    å‡ºåŠ›ä¾‹: "2025/12/25 14:24:00"
    """
    match = re.search(r"(\d{4})-(\d{2})-(\d{2})\s(\d{2}):(\d{2})", date_text)
    if match:
        y, m, d, H, M = match.groups()
        return f"{y}/{m}/{d} {H}:{M}:00"
    
    return datetime.datetime.now().strftime('%Y/%m/%d %H:%M:%S')

def scrape_blog_comments():
    print("ğŸŒ ãƒ–ãƒ­ã‚°ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
    soup = get_soup(BLOG_URL)
    if not soup:
        return []

    # æœ€æ–°è¨˜äº‹URLã‚’å–å¾—
    article_urls = []
    
    for a in soup.find_all("a", href=True):
        href = a['href']
        if "hisakobaab.exblog.jp" in href and re.search(r'/\d+/', href):
            if "#" not in href:
                if href not in article_urls:
                    article_urls.append(href)
        
        if len(article_urls) >= LATEST_ARTICLE_LIMIT:
            break
            
    print(f"ğŸ“„ ç›´è¿‘ {len(article_urls)} ä»¶ã®è¨˜äº‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚")

    comments_found = []

    for url in article_urls:
        art_soup = get_soup(url)
        if not art_soup:
            continue

        tails = art_soup.select(".COMMENT_TAIL")

        for tail in tails:
            try:
                # 1. åå‰å–å¾—
                name_tag = tail.select_one("b")
                author_name = name_tag.text.strip() if name_tag else "åç„¡ã—"

                # 2. æ—¥ä»˜å–å¾— (ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›æ¸ˆã¿)
                tail_text = tail.get_text()
                formatted_date = parse_excite_date(tail_text)

                # 3. æœ¬æ–‡å–å¾—
                body_div = tail.find_next_sibling("div", class_="COMMENT_BODY")
                
                if body_div:
                    for tool in body_div.select(".xbg-comment-tools"):
                        tool.decompose() 
                    
                    message_body = body_div.get_text("\n").strip()

                    if message_body:
                        # ã€ä¿®æ­£ã€‘åå‰ã®å¾Œã‚ã® (ãƒ–ãƒ­ã‚°ã‚ˆã‚Š) ã‚’å‰Šé™¤ã—ã¾ã—ãŸ
                        comments_found.append({
                            "timestamp": formatted_date,
                            "name": author_name, 
                            "message": message_body
                        })
            except Exception as e:
                print(f"è§£æã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        time.sleep(1) 

    return comments_found

def update_spreadsheet(new_comments):
    if not os.path.exists(SERVICE_ACCOUNT_FILE):
        print("\nâš ï¸ ã€é‡è¦ã€‘ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼")
        return

    print("ğŸ“Š ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ¥ç¶šä¸­...")
    try:
        scope = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']
        creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=scope)
        client = gspread.authorize(creds)
        
        sheet = client.open_by_key(SPREADSHEET_KEY).sheet1
        
        existing_rows = sheet.get_all_values()
        
        existing_signatures = set()
        for row in existing_rows:
            if len(row) >= 3:
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨
                sig = f"{row[0]}_{row[1]}_{row[2]}"
                existing_signatures.add(sig)

        rows_to_add = []
        for comment in new_comments:
            row_data = [comment["timestamp"], comment["name"], comment["message"]]
            sig = f"{row_data[0]}_{row_data[1]}_{row_data[2]}"
            
            if sig not in existing_signatures:
                rows_to_add.append(row_data)
                existing_signatures.add(sig)

        if rows_to_add:
            print(f"ğŸš€ {len(rows_to_add)} ä»¶ã®æ–°è¦ã‚³ãƒ¡ãƒ³ãƒˆã‚’æ›¸ãè¾¼ã¿ã¾ã™...")
            sheet.append_rows(rows_to_add)
            print("âœ… æ›¸ãè¾¼ã¿å®Œäº†")
        else:
            print("âœ¨ æ–°ã—ã„ã‚³ãƒ¡ãƒ³ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    except Exception as e:
        print(f"âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    print("=== ãƒ–ãƒ­ã‚°ã‚³ãƒ¡ãƒ³ãƒˆåé›†é–‹å§‹ ===")
    comments = scrape_blog_comments()
    print(f"ğŸ” ã‚¹ã‚­ãƒ£ãƒ³çµæœ: åˆè¨ˆ {len(comments)} ä»¶ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’æ¤œå‡º")
    
    if comments:
        update_spreadsheet(comments)
    
    print("=== å®Œäº† ===")

if __name__ == "__main__":
    main()
